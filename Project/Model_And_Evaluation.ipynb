{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b86c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from Data_Pre_Process import pre_process_corpus, tfidf, bert_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7812f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the preprocessed dataset\n",
    "DF = pd.read_csv('my_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d82d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=DF[:round(0.7*len(DF))]\n",
    "df_test=DF[round(0.7*len(DF)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56966cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert encoded data\n",
    "X = np.load('encoded_train_tweet.npy')\n",
    "X_test = np.load('encoded_test_tweet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f7cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tfidf encoded data\n",
    "tfidf_train = np.load(\"tfidf_train_tweet.npy\")\n",
    "tfidf_test = np.load(\"tfidf_test_tweet.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22447a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e97486",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df_train.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e919a",
   "metadata": {},
   "source": [
    "# DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827eb0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 128)               4214272   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4247425 (16.20 MB)\n",
      "Trainable params: 4247425 (16.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "44/44 [==============================] - 13s 255ms/step - loss: 1.7887 - acc: 0.9203 - val_loss: 1.0830 - val_acc: 0.9290\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.7345 - acc: 0.9317 - val_loss: 0.4464 - val_acc: 0.9396\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.3161 - acc: 0.9386 - val_loss: 0.2167 - val_acc: 0.9455\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1931 - acc: 0.9447 - val_loss: 0.1732 - val_acc: 0.9467\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1717 - acc: 0.9473 - val_loss: 0.1756 - val_acc: 0.9460\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.1626 - acc: 0.9490 - val_loss: 0.1598 - val_acc: 0.9493\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1583 - acc: 0.9497 - val_loss: 0.1566 - val_acc: 0.9486\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.1539 - acc: 0.9513 - val_loss: 0.1530 - val_acc: 0.9504\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1502 - acc: 0.9528 - val_loss: 0.1495 - val_acc: 0.9521\n",
      "Epoch 10/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1487 - acc: 0.9528 - val_loss: 0.1518 - val_acc: 0.9510\n",
      "Epoch 11/50\n",
      "44/44 [==============================] - 5s 119ms/step - loss: 0.1457 - acc: 0.9532 - val_loss: 0.1514 - val_acc: 0.9517\n",
      "Epoch 12/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.1437 - acc: 0.9545 - val_loss: 0.1474 - val_acc: 0.9507\n",
      "Epoch 13/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.1414 - acc: 0.9566 - val_loss: 0.1461 - val_acc: 0.9515\n",
      "Epoch 14/50\n",
      "44/44 [==============================] - 5s 124ms/step - loss: 0.1385 - acc: 0.9553 - val_loss: 0.1556 - val_acc: 0.9464\n",
      "Epoch 15/50\n",
      "44/44 [==============================] - 6s 133ms/step - loss: 0.1366 - acc: 0.9564 - val_loss: 0.1681 - val_acc: 0.9502\n",
      "Epoch 16/50\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.1362 - acc: 0.9573 - val_loss: 0.1437 - val_acc: 0.9547\n",
      "Epoch 17/50\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 0.1348 - acc: 0.9581 - val_loss: 0.1412 - val_acc: 0.9557\n",
      "Epoch 18/50\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 0.1332 - acc: 0.9574 - val_loss: 0.1452 - val_acc: 0.9535\n",
      "Epoch 19/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.1301 - acc: 0.9592 - val_loss: 0.1597 - val_acc: 0.9544\n",
      "Epoch 20/50\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.1284 - acc: 0.9593 - val_loss: 0.1620 - val_acc: 0.9542\n",
      "Epoch 21/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.1295 - acc: 0.9600 - val_loss: 0.1427 - val_acc: 0.9553\n",
      "Epoch 22/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1263 - acc: 0.9601 - val_loss: 0.1484 - val_acc: 0.9560\n",
      "Epoch 23/50\n",
      "44/44 [==============================] - 6s 128ms/step - loss: 0.1240 - acc: 0.9612 - val_loss: 0.1431 - val_acc: 0.9569\n",
      "Epoch 24/50\n",
      "44/44 [==============================] - 5s 122ms/step - loss: 0.1214 - acc: 0.9629 - val_loss: 0.1463 - val_acc: 0.9573\n",
      "Epoch 25/50\n",
      "44/44 [==============================] - 6s 147ms/step - loss: 0.1216 - acc: 0.9625 - val_loss: 0.1494 - val_acc: 0.9565\n",
      "Epoch 26/50\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.1184 - acc: 0.9640 - val_loss: 0.1383 - val_acc: 0.9557\n",
      "Epoch 27/50\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.1191 - acc: 0.9634 - val_loss: 0.1377 - val_acc: 0.9581\n",
      "Epoch 28/50\n",
      "44/44 [==============================] - 5s 119ms/step - loss: 0.1148 - acc: 0.9659 - val_loss: 0.1396 - val_acc: 0.9580\n",
      "Epoch 29/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1147 - acc: 0.9647 - val_loss: 0.1474 - val_acc: 0.9581\n",
      "Epoch 30/50\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 0.1171 - acc: 0.9652 - val_loss: 0.1583 - val_acc: 0.9473\n",
      "Epoch 31/50\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.1118 - acc: 0.9664 - val_loss: 0.1816 - val_acc: 0.9509\n",
      "Epoch 32/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.1112 - acc: 0.9681 - val_loss: 0.1478 - val_acc: 0.9579\n",
      "Epoch 33/50\n",
      "44/44 [==============================] - 5s 124ms/step - loss: 0.1087 - acc: 0.9683 - val_loss: 0.1365 - val_acc: 0.9576\n",
      "Epoch 34/50\n",
      "44/44 [==============================] - 6s 129ms/step - loss: 0.1089 - acc: 0.9680 - val_loss: 0.1446 - val_acc: 0.9571\n",
      "Epoch 35/50\n",
      "44/44 [==============================] - 6s 126ms/step - loss: 0.1057 - acc: 0.9692 - val_loss: 0.1441 - val_acc: 0.9586\n",
      "Epoch 36/50\n",
      "44/44 [==============================] - 6s 127ms/step - loss: 0.1060 - acc: 0.9697 - val_loss: 0.1663 - val_acc: 0.9559\n",
      "Epoch 37/50\n",
      "44/44 [==============================] - 6s 127ms/step - loss: 0.1043 - acc: 0.9692 - val_loss: 0.1429 - val_acc: 0.9587\n",
      "Epoch 38/50\n",
      "44/44 [==============================] - 5s 124ms/step - loss: 0.1017 - acc: 0.9709 - val_loss: 0.1372 - val_acc: 0.9575\n",
      "Epoch 39/50\n",
      "44/44 [==============================] - 5s 123ms/step - loss: 0.1026 - acc: 0.9699 - val_loss: 0.1396 - val_acc: 0.9598\n",
      "Epoch 40/50\n",
      "44/44 [==============================] - 5s 119ms/step - loss: 0.1014 - acc: 0.9708 - val_loss: 0.1362 - val_acc: 0.9576\n",
      "Epoch 41/50\n",
      "44/44 [==============================] - 5s 119ms/step - loss: 0.0971 - acc: 0.9723 - val_loss: 0.1398 - val_acc: 0.9575\n",
      "Epoch 42/50\n",
      "44/44 [==============================] - 5s 123ms/step - loss: 0.0994 - acc: 0.9713 - val_loss: 0.1338 - val_acc: 0.9589\n",
      "Epoch 43/50\n",
      "44/44 [==============================] - 7s 151ms/step - loss: 0.0958 - acc: 0.9735 - val_loss: 0.1813 - val_acc: 0.9524\n",
      "Epoch 44/50\n",
      "44/44 [==============================] - 6s 142ms/step - loss: 0.0941 - acc: 0.9741 - val_loss: 0.1706 - val_acc: 0.9583\n",
      "Epoch 45/50\n",
      "44/44 [==============================] - 6s 142ms/step - loss: 0.0938 - acc: 0.9744 - val_loss: 0.1374 - val_acc: 0.9593\n",
      "Epoch 46/50\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.0903 - acc: 0.9754 - val_loss: 0.1726 - val_acc: 0.9554\n",
      "Epoch 47/50\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.0901 - acc: 0.9747 - val_loss: 0.1411 - val_acc: 0.9558\n",
      "Epoch 48/50\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.0889 - acc: 0.9758 - val_loss: 0.1361 - val_acc: 0.9590\n",
      "Epoch 49/50\n",
      "44/44 [==============================] - 5s 123ms/step - loss: 0.0898 - acc: 0.9745 - val_loss: 0.1464 - val_acc: 0.9612\n",
      "Epoch 50/50\n",
      "44/44 [==============================] - 5s 116ms/step - loss: 0.0885 - acc: 0.9760 - val_loss: 0.1366 - val_acc: 0.9585\n"
     ]
    }
   ],
   "source": [
    "# Configure early stopping based on validation accuracy with a patience of 10 epochs and restore the best weights\n",
    "callback = EarlyStopping(monitor='val_acc', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Build a sequential neural network model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add dense layers with dropout and L2 regularization to prevent overfitting\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(np.hstack((tfidf_train, X)).shape[1],)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer and binary crossentropy loss function\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Output the model summary to understand the architecture\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model using combined TF-IDF features and additional features 'X', with callbacks for early stopping\n",
    "history = model.fit(np.hstack((tfidf_train, X)), y, epochs=50, batch_size=512, \n",
    "                    validation_data=(np.hstack((tfidf_test, X_test)), y_test), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471fe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('dnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14016d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 128)               4214272   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4247425 (16.20 MB)\n",
      "Trainable params: 4247425 (16.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "dnn_model = load_model('dnn_model.keras')\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "400ce300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the F1 score for model predictions above a specified threshold\n",
    "def get_f1_score(model, test_data, true_labels, threshold=0.5):\n",
    "    \n",
    "    pred= model.predict(test_data)\n",
    "    y_hat= np.zeros_like(pred)\n",
    "    y_hat_idx= np.where(pred>threshold)[0]\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        if i in y_hat_idx:\n",
    "            y_hat[i]=1\n",
    "    \n",
    "    return f1_score(true_labels, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "323cc5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 7ms/step\n",
      "300/300 [==============================] - 2s 7ms/step\n",
      "300/300 [==============================] - 2s 7ms/step\n",
      "300/300 [==============================] - 3s 10ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 7ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 7ms/step\n",
      "300/300 [==============================] - 2s 7ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 2s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 8ms/step\n",
      "300/300 [==============================] - 3s 10ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "300/300 [==============================] - 3s 9ms/step\n",
      "Optimal Threshold: 0.42\n"
     ]
    }
   ],
   "source": [
    "# calculating the optimal threshold for our model\n",
    "f1_scores = [get_f1_score(model, np.hstack((tfidf_test, X_test)), y_test, threshold/100) for threshold in range(90)]\n",
    "f1_scores = np.array(f1_scores)\n",
    "optimal_threshold = np.argmax(f1_scores) / 100\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b11b9d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6935123042505593"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the F1 score corresponding to the optimal threshold from the dictionary 'f1_scores'\n",
    "f1_scores[int(optimal_threshold * 100)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161072a",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e68eb466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with TF-IDF only: 0.42403628117913833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with combined features: 0.6330196749358425\n"
     ]
    }
   ],
   "source": [
    "# Initialize a logistic regression model and train it using only TF-IDF features\n",
    "log_reg = LogisticRegression()\n",
    "tfidf_test_dense = np.asarray(tfidf_test)  \n",
    "log_reg.fit(tfidf_train, y) \n",
    "\n",
    "# Predict and evaluate using the TF-IDF features on the test set\n",
    "predictions_tfidf_only = log_reg.predict(tfidf_test)\n",
    "f1_tfidf_only = f1_score(y_test, predictions_tfidf_only) \n",
    "print(\"F1 Score with TF-IDF only:\", f1_tfidf_only) \n",
    "\n",
    "# Retrain the logistic regression model using combined TF-IDF and additional features\n",
    "log_reg.fit(np.hstack((tfidf_train, X)), y) \n",
    "predictions_combined = log_reg.predict(np.hstack((tfidf_test, X_test))) \n",
    "f1_combined = f1_score(y_test, predictions_combined)\n",
    "print(\"F1 Score with combined features:\", f1_combined) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b0aa8",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a RandomForestClassifier and train it using only TF-IDF features\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(tfidf_train, y)  \n",
    "\n",
    "# Predict using the TF-IDF features on the test set and calculate the F1 score\n",
    "predictions_tfidf_only = rf_classifier.predict(tfidf_test)\n",
    "f1_tfidf_only = f1_score(y_test, predictions_tfidf_only)\n",
    "print(\"F1 Score with TF-IDF only:\", f1_tfidf_only)  \n",
    "\n",
    "# Retrain the RandomForestClassifier using combined TF-IDF and additional features\n",
    "rf_classifier.fit(np.hstack((tfidf_train, X)), y)  \n",
    "predictions_combined = rf_classifier.predict(np.hstack((tfidf_test, X_test))) \n",
    "f1_combined = f1_score(y_test, predictions_combined)  \n",
    "print(\"F1 Score with combined features:\", f1_combined) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0867d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify input text as hate speech or not using a pre-trained deep neural network (DNN) model\n",
    "def classify_text():\n",
    "\n",
    "    input_message = input(\"Enter a text to classify as hate speech or not: \")\n",
    "    input_text = pd.Series([input_message]) \n",
    "\n",
    "    preprocessed_text_tfidf = pre_process_corpus(input_text)[0]\n",
    "    tfidf_features = tfidf.transform([preprocessed_text_tfidf]).todense()\n",
    "\n",
    "    bert_features = bert_encoded.encode([preprocessed_text_tfidf])\n",
    "\n",
    "    combined_features = np.hstack((tfidf_features, bert_features))\n",
    "\n",
    "    prediction = dnn_model.predict(combined_features)\n",
    "    predicted_class = 'Hate Speech' if prediction[0] >= 0.5 else 'Not Hate Speech'  \n",
    "\n",
    "    print(f\"Prediction: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82538a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a text to classify as hate speech or not: It's just a fact that Black people are less intelligent than whites\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Prediction: Hate Speech\n"
     ]
    }
   ],
   "source": [
    "classify_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514a5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
