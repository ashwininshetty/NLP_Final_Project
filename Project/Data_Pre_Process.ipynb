{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa2d6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_corpus(corpus):\n",
    "    # Initialize a lemmatizer and create a set of English stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('English'))\n",
    "    \n",
    "    # Convert all text in the corpus to lowercase and split into words\n",
    "    corpus = corpus.apply(lambda x: x.lower().split())\n",
    "    \n",
    "    # Filter out single-character words from each article\n",
    "    corpus = corpus.apply(lambda article: [word for word in article if len(word) > 1])\n",
    "    \n",
    "    # Exclude words that are purely numeric\n",
    "    corpus = corpus.apply(lambda article: [word for word in article if not word.isnumeric()])\n",
    "    \n",
    "    # Apply lemmatization to reduce words to their base or dictionary form\n",
    "    corpus = corpus.apply(lambda article: [lemmatizer.lemmatize(word) for word in article])\n",
    "    \n",
    "    # Remove stop words to reduce noise in the text\n",
    "    corpus = corpus.apply(lambda article: [word for word in article if word not in stop_words])\n",
    "    \n",
    "    # Eliminate mentions (words containing '@') from the text\n",
    "    corpus = corpus.apply(lambda article: [word for word in article if '@' not in word])\n",
    "    \n",
    "    # Rejoin words into a single string per article after processing\n",
    "    corpus = corpus.apply(lambda article: ' '.join(article))\n",
    "    \n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1cdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the train.csv\n",
    "DF=pd.read_csv('train.csv', usecols=['label', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8106009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0,\n",
       "        ' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'],\n",
       "       [0,\n",
       "        \"@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\"],\n",
       "       [0, '  bihday your majesty'],\n",
       "       [0,\n",
       "        '#model   i love u take with u all the time in urð\\x9f\\x93±!!! ð\\x9f\\x98\\x99ð\\x9f\\x98\\x8eð\\x9f\\x91\\x84ð\\x9f\\x91\\x85ð\\x9f\\x92¦ð\\x9f\\x92¦ð\\x9f\\x92¦  '],\n",
       "       [0, ' factsguide: society now    #motivation'],\n",
       "       [0,\n",
       "        '[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  '],\n",
       "       [0,\n",
       "        ' @user camping tomorrow @user @user @user @user @user @user @user dannyâ\\x80¦'],\n",
       "       [0,\n",
       "        \"the next school year is the year for exams.ð\\x9f\\x98¯ can't think about that ð\\x9f\\x98\\xad #school #exams   #hate #imagine #actorslife #revolutionschool #girl\"],\n",
       "       [0,\n",
       "        'we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â\\x80¦ '],\n",
       "       [0, \" @user @user welcome here !  i'm   it's so #gr8 ! \"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get first 10 rows from the dataframe\n",
    "DF.head(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81448a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to pre-process the data on the tweet column\n",
    "DF.tweet=pre_process_corpus(DF.tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b33b085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drag kid dysfunct...\n",
       "1    thanks #lyft credit can't use cause offer whee...\n",
       "2                                       bihday majesty\n",
       "3    #model love take time urð±!!! ðððð...\n",
       "4                      factsguide: society #motivation\n",
       "5    [2/2] huge fan fare big talking leave. chaos p...\n",
       "6                            camping tomorrow dannyâ¦\n",
       "7    next school year year exams.ð¯ can't think ð...\n",
       "8    won!!! love land!!! #allin #cavs #champions #c...\n",
       "9                                     welcome i'm #gr8\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.tweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe72bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data back to the CSV\n",
    "DF.to_csv('my_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb53006",
   "metadata": {},
   "source": [
    "# Training and testing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fde2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame 'DF' into a training set and a test set with a 70/30 ratio\n",
    "df_train = DF[:round(0.7 * len(DF))]\n",
    "df_test = DF[round(0.7 * len(DF)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e9b4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices are reset to avoid issues during look-ups\n",
    "df_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a1d30",
   "metadata": {},
   "source": [
    "# Embeddings, Bert and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57701824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model with 'distilbert-base-nli-mean-tokens' for encoding sentences\n",
    "bert_encoded = SentenceTransformer('distilbert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5eff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'tweet' column of the training DataFrame using the BERT model and save the encoded data as a numpy file\n",
    "df_train_encoded = bert_encoded.encode(df_train.tweet)\n",
    "np.save('encoded_train_tweet.npy', df_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "969a626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat same for test\n",
    "df_test_enocded = bert_encoded.encode(df_test.tweet) \n",
    "np.save('encoded_test_tweet.npy', df_test_enocded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8fe4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the TF-IDF vectorizer on the 'tweet' column of the training data and transform it into a dense matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_train_df_dense = tfidf.fit_transform(df_train.tweet).todense()\n",
    "\n",
    "# Transform the 'tweet' column of the test data using the fitted TF-IDF vectorizer into a dense matrix\n",
    "tfidf_test_df_dense = tfidf.transform(df_test.tweet).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "700f294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dense TF-IDF matrices of the training and test tweets to numpy files\n",
    "np.save('tfidf_train_tweet.npy', tfidf_train_df_dense)\n",
    "np.save('tfidf_test_tweet.npy', tfidf_test_df_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a2aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
